"""
title: advanced_mcts
author: angrysky56
author_url: https://github.com/angrysky56
description: Advanced Monte Carlo Tree Search with persistent visualization and strategic memory
version: 0.3.3

Key improvements in v0.3.3:
- Integrated fixes for syntax errors and duplicated code.
- Corrected TypeError by adding robust None checks and defaults for LLM outputs.
- Ensured chat history persistence with emit_message.
- Applied minimal Mermaid escaping for potentially better rendering.
- Kept and verified enhanced exploration logic.
- Kept JSON export functionality.
- Removed unsafe resource cleanup attempts.
- Added more error handling around LLM calls and formatting.
- Set reasonably high default exploration parameters assuming Valves are broken.
"""

from fastapi import Request
import logging
import random
import math
import asyncio
import json
import re
import html
import numpy as np
from scipy import stats
from sklearn.metrics.pairwise import cosine_similarity
from typing import (
    List,
    Optional,
    AsyncGenerator,
    Callable,
    Awaitable,
    Generator,
    Iterator,
    Dict,
    Any,
    Tuple,
    Set,
)
from pydantic import BaseModel, Field
from open_webui.constants import TASKS
import open_webui.routers.ollama as ollama
from open_webui.main import app

# ==============================================================================

name = "advanced_mcts"

# Configurable parameters with default values - Adjusted for better exploration
# Assuming Valves might not work, these defaults are important.
config = {
    "max_children": 4,
    "exploration_weight": 2.5, # Higher default exploration
    "max_iterations": 3,
    "simulations_per_iteration": 15, # Higher default simulations
    "thoughts_per_node": 2, # Legacy
    "surprise_threshold": 0.7,
    "use_semantic_distance": True,
    "use_llm_embedding_for_distance": False,
    "node_label_length": 100,
    "relative_evaluation": True,
    "score_diversity_bonus": 0.7,
    "force_exploration_interval": 4, # Interval for non-leaf selection
    "debug_logging": True, # Enable detailed logging
    "global_context_in_prompts": True,
    "track_explored_approaches": True,
    "sibling_awareness": True,
    "memory_cutoff": 5,
    "early_stopping": True,
    "early_stopping_threshold": 9.0,
    "early_stopping_stability": 3,
}

# ==============================================================================

# Prompt templates (using versions from v0.3.1)
thoughts_prompt = """
<instruction>
You are generating a thought for improving a draft answer in a strategic exploration process.
</instruction>

<context>
CURRENT EXPLORATION STATUS:
- Best answer found so far (score: {best_score}/10): 
{best_answer}

- Previously explored approaches: 
{explored_approaches}

- Current approach type: {current_approach}

- Current draft to improve:
{answer}
</context>

<question>
{question}
</question>

Based on the context provided, suggest a SIGNIFICANTLY DIFFERENT APPROACH or identify a MAJOR WEAKNESS to address.
Consider:
- Approaches that haven't been tried yet
- Ways to bridge the gap between this draft and the best answer found
- Completely different frameworks or perspectives 
- Critical flaws in the current draft compared to the best answer

YOUR REPLY SHOULD BE A SINGLE SENTENCE THAT SUGGESTS A FUNDAMENTALLY DIFFERENT DIRECTION.
"""

relative_eval_prompt = """
<instruction>
Compare this new answer to the previous one and rate the IMPROVEMENT, considering the global context.
</instruction>

<context>
CURRENT EXPLORATION STATUS:
- Best answer found so far (score: {best_score}/10): 
{best_answer}
</context>

<question>
{question}
</question>

<previous_answer>
{parent_answer}
</previous_answer>

<new_answer>
{answer}
</new_answer>

Rate the IMPROVEMENT of the new answer over the previous answer on a scale of 1 to 5:
1: Worse or irrelevant change.
2: No real improvement, maybe slightly different wording.
3: Minor improvement, slightly clearer or adds a small relevant detail.
4: Clear improvement, addresses a flaw or adds significant value.
5: Major improvement, fundamentally better approach.

Consider both the improvement over the parent AND how it compares to the best answer found so far.
Focus ONLY on the improvement quality. Reply with a single number 1-5.
"""

eval_answer_prompt = """
<instruction>
Evaluate this answer critically, considering both the question and the search context.
</instruction>

<context>
CURRENT EXPLORATION STATUS:
- Best answer found so far (score: {best_score}/10): 
{best_answer}
</context>

<question>
{question}
</question>

<answer_to_evaluate>
{answer}
</answer_to_evaluate>

Critically evaluate the answer on a scale of 1 to 10.
- 1-3: Fundamentally flawed, irrelevant, or very incomplete.
- 4-6: Partially addresses the question but has significant issues or omissions.
- 7-8: Good answer, mostly correct and relevant, minor issues possible.
- 9-10: Excellent, comprehensive, accurate, and well-structured answer.

Be STRICT in your evaluation. Do not give high scores (9-10) unless the answer is truly exceptional.
Compare it to the best answer found so far, if available.
Reply with a single number between 1 and 10 only. Do not write anything else.
THINK CAREFULLY AND USE THE FULL SCALE.
"""

analyze_prompt = """
<instruction>
Analyze the current iteration of the Monte Carlo Tree Search process.
</instruction>

<context>
SEARCH STATISTICS:
- Original question: {question}
- Best answer found: {best_answer}
- Best score achieved: {best_score}
- Current tree depth: {tree_depth}
- Number of branches: {branches}
- Explored approach types: {approach_types}
</context>

Analyze this iteration of the thought process. Consider the following:
1. What aspects of the best answer made it successful?
2. What patterns or approaches led to higher-scoring thoughts?
3. Were there any common pitfalls or irrelevant tangents in lower-scoring thoughts?
4. How can the thought generation process be improved for the next iteration?
5. What approaches or dimensions have NOT been explored yet?

Provide a concise analysis and suggest one specific improvement strategy for the next iteration.
"""

update_prompt = """
<instruction>
You are refining an answer based on strategic critique, with awareness of the overall search process.
</instruction>

<context>
CURRENT EXPLORATION STATUS:
- Best answer found so far (score: {best_score}/10): 
{best_answer}

- Previously explored approaches: 
{explored_approaches}

- Current approach type: {current_approach}
</context>

<question>
{question}
</question>

<draft>
{answer}
</draft>

<critique>
{improvements}
</critique>

The critique suggests a SIGNIFICANTLY DIFFERENT APPROACH or identifies a MAJOR WEAKNESS.
Completely rethink your approach based on this critique and the global context provided.
WRITE A SUBSTANTIALLY REVISED ANSWER THAT:
1. Takes a different angle from the current draft
2. Addresses the critique directly
3. Attempts to improve upon the best answer found so far
4. Explores territory not covered by previously explored approaches
"""

initial_prompt = """
<instruction>
Answer the question below. Do not pay attention to unexpected casing, punctuation or accent marks.
</instruction>

<question>
{question}
</question>
"""

embedding_prompt = """
<instruction>
Create a compact semantic embedding description of the following text. 
Identify the key concepts, ideas, and approach used in this answer.
Your response should be a short comma-separated list of key terms and concepts.
</instruction>

<text>
{text}
</text>
""".strip()

final_summary_prompt = """
<instruction>
Create a comprehensive final summary of the Monte Carlo Tree Search process and its findings.
</instruction>

<context>
SEARCH STATISTICS:
- Original question: {question}
- Best answer found (score: {best_score}/10): 
{best_answer}
- Number of iterations completed: {iterations}
- Total nodes explored: {total_nodes}
- Tree depth: {tree_depth}
- Number of approaches tried: {approach_count}
- Most successful approach: {best_approach} (avg score: {best_approach_score})
</context>

Write a comprehensive final report that includes:
1. A brief overview of the search process
2. The best answer found and why it was successful
3. Key insights from the exploration process
4. Alternative approaches that were considered but scored lower
5. A conclusion summarizing the essential findings

This should be a polished final output that synthesizes all the learning from the search process.
"""

# ==============================================================================

# Logger Setup Function
def setup_logger():
    logger = logging.getLogger(__name__)
    log_level = logging.DEBUG if config.get("debug_logging", True) else logging.INFO
    logger.setLevel(log_level)
    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.set_name(name)
        formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.propagate = False
    # Ensure existing handlers also have the correct level
    for handler in logger.handlers:
        handler.setLevel(log_level)
    return logger

# Global Logger Instance
logger = setup_logger()

# Admin User Mock
class AdminUserMock:
    def __init__(self):
        self.role = "admin"
admin = AdminUserMock()

# ==============================================================================

# Helper functions (Not currently used)
# def modify_text(text, percentage): ...
# def replace_with_mapping(text, mapping): ...

# ==============================================================================

# Text processing functions
def truncate_text(text, max_length=None):
    """Truncate text and add ellipsis if needed."""
    if not text: return ""
    text = str(text) # Ensure text is string
    if max_length is None: max_length = config["node_label_length"]
    text = text.replace("\n", " ").strip()
    if len(text) <= max_length: return text
    return text[:max_length] + "..."

def escape_mermaid(text):
    """Minimal escaping for Mermaid labels."""
    if not text: return ""
    text = str(text) # Ensure text is string
    # Replace characters known to break Mermaid syntax within labels
    # Primarily double quotes and backticks
    return text.replace("`", "'").replace('"', "'")

def calculate_semantic_distance(text1, text2, llm=None):
    """Calculate semantic distance using TF-IDF or fallback."""
    if not text1 or not text2: return 1.0 # Max distance if one is empty
    # Ensure inputs are strings
    text1 = str(text1)
    text2 = str(text2)

    # LLM embedding logic placeholder
    if config["use_llm_embedding_for_distance"] and llm: pass

    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    vectorizer = TfidfVectorizer()
    try:
        tfidf_matrix = vectorizer.fit_transform([text1, text2])
        if tfidf_matrix.shape[0] < 2 or tfidf_matrix.shape[1] == 0: raise ValueError("TF-IDF matrix issue.")
        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
        similarity = max(0.0, min(1.0, similarity)) # Clamp value
        return 1.0 - similarity
    except Exception as e:
        logger.warning(f"TF-IDF semantic distance error: {e}. Falling back to Jaccard.")
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))
        if not words1 or not words2: return 1.0
        intersection = len(words1.intersection(words2))
        union = len(words1.union(words2))
        if union == 0: return 0.0
        jaccard_similarity = intersection / union
        return 1.0 - jaccard_similarity

# ==============================================================================

class Node:
    """A node in the Monte Carlo Tree Search."""
    def __init__(self, **kwargs):
        self.id = "node_" + "".join(random.choices("abcdefghijklmnopqrstuvwxyz", k=4))
        self.content = str(kwargs.get("content", "")) # Ensure string
        self.parent = kwargs.get("parent")
        self.exploration_weight = kwargs.get("exploration_weight", config["exploration_weight"])
        self.max_children = kwargs.get("max_children", config["max_children"])
        self.children = []
        self.visits = 0
        self.value = 0.0 # Ensure float
        self.raw_scores = []
        self.sequence = kwargs.get("sequence", 0)
        self.embedding = "" # Placeholder
        self.is_surprising = kwargs.get("is_surprising", False)
        self.surprise_explanation = str(kwargs.get("surprise_explanation", "")) # Ensure string
        self.approach_type = str(kwargs.get("approach_type", "initial")) # Ensure string
        self.thought = str(kwargs.get("thought", "")) # Ensure string
        self.generation_context = kwargs.get("generation_context", {})

        if self.is_surprising:
            self.value = 2.0 # Optimistic bias

    def add_child(self, child: "Node"):
        child.parent = self
        self.children.append(child)
        return child

    def fully_expanded(self):
        return len(self.children) >= self.max_children

    def uct_value(self):
        epsilon = 1e-6
        if self.visits == 0: return float("inf")
        exploitation = self.value / self.visits
        parent_visits = self.parent.visits if self.parent else 1
        exploration = self.exploration_weight * math.sqrt(math.log(parent_visits + epsilon) / self.visits)
        surprise_bonus = 0.3 if self.is_surprising else 0
        diversity_bonus = 0.0
        # Diversity bonus calculation (ensure division by zero is avoided)
        if self.parent and len(self.parent.children) > 1 and config["score_diversity_bonus"] > 0:
            sibling_scores = [c.value / max(1, c.visits) for c in self.parent.children if c != self and c.visits > 0]
            num_valid_siblings = len(sibling_scores)
            if num_valid_siblings > 0:
                sibling_avg_value = sum(sibling_scores) / num_valid_siblings
                my_avg_value = self.value / self.visits
                score_diff = abs(my_avg_value - sibling_avg_value)
                diversity_bonus = config["score_diversity_bonus"] * score_diff
        return exploitation + exploration + surprise_bonus + diversity_bonus

    def mermaid(self, offset=0, selected=None):
        padding = " " * offset
        score = self.value / max(1, self.visits)
        score_text = f"{score:.2f}" if self.visits > 0 else "N/A"
        approach_label = f" [{self.approach_type}]" if self.approach_type != "initial" else ""
        display_text = truncate_text(self.content)
        surprise_indicator = "⭐ " if self.is_surprising else ""
        node_label_content = escape_mermaid(display_text) # Use minimal escaping
        node_label = f'{self.id}["{surprise_indicator}Node {self.sequence}: Score={score_text}{approach_label}<br/>\'{node_label_content}\'"]'
        msg = f"{padding}{node_label}\n"
        # Styling logic...
        if selected and selected == self.id: msg += f"{padding}style {self.id} fill:#d4f0fd,stroke:#0099ff,stroke-width:2px\n"
        elif self.visits > 0:
             if self.is_surprising: msg += f"{padding}style {self.id} fill:#fcf8d3,stroke:#f1c40f,stroke-width:2px\n"
             elif score > 7: msg += f"{padding}style {self.id} fill:#d4ffd4,stroke:#00cc00\n"
             elif score < 4: msg += f"{padding}style {self.id} fill:#ffd4d4,stroke:#cc0000\n"
        # Children rendering...
        for child in self.children:
            msg += child.mermaid(offset + 4, selected)
            edge_label = f"{child.visits}"
            if hasattr(child, "thought") and child.thought:
                thought_preview = truncate_text(child.thought, 20)
                safe_thought_preview = escape_mermaid(thought_preview)
                edge_label = f"{edge_label}|{safe_thought_preview}"
            msg += f"{padding}{self.id} -->|{edge_label}| {child.id}\n"
        return msg

    def best_child(self):
        if not self.children: return self
        return max(self.children, key=lambda child: (child.visits, child.value / max(1, child.visits)))

    def node_to_json(self):
        # Convert node and children to JSON serializable dict
        return {
            "id": self.id, "sequence": self.sequence,
            "content": truncate_text(self.content, 200), "visits": self.visits,
            "value": round(self.value, 2), "score": round(self.value / max(1, self.visits), 2),
            "approach_type": self.approach_type, "is_surprising": self.is_surprising,
            "thought": truncate_text(self.thought, 100) if hasattr(self, "thought") else "",
            "children": [child.node_to_json() for child in self.children]
        }

# ==============================================================================

class MCTS:
    """Monte Carlo Tree Search implementation with strategic memory."""
    def __init__(self, **kwargs):
        self.question = kwargs.get("question")
        self.root = kwargs.get("root")
        self.node_sequence = 0
        self.root.sequence = self.get_next_sequence()
        self.llm = kwargs.get("llm")
        self.selected = self.root
        self.exploration_weight = config["exploration_weight"]
        self.thought_history = []
        self.debug_history = []
        self.surprising_nodes = []
        self.best_solution = str(self.root.content) # Ensure string
        self.best_score = 0.0 # Ensure float
        self.iterations_completed = 0
        self.simulations_completed = 0
        self.high_score_counter = 0
        self.random_state = random.Random()
        self.approach_types = ["initial"]
        self.explored_approaches = {}
        self.explored_thoughts = set()
        self.approach_scores = {}
        self.memory = {"depth": 0, "branches": 0, "high_scoring_nodes": [], "repeated_approaches": []}
        self.iteration_json_snapshots = []
        self.thought_history.append(f"# MCTS Analysis for Question: {self.question}\n\nStarting exploration...\n")

    def get_next_sequence(self):
        self.node_sequence += 1
        return self.node_sequence

    def export_tree_as_json(self):
        return self.root.node_to_json()

    def get_context_for_node(self, node):
        # Builds context, handling potential None/missing values gracefully
        best_answer_str = str(self.best_solution) if self.best_solution else "N/A"
        context = {
            "best_answer": best_answer_str,
            "best_score": self.best_score,
            "current_approach": getattr(node, 'approach_type', 'initial'),
            "tree_depth": self.memory.get("depth", 0),
            "branches": self.memory.get("branches", 0),
            "approach_types": ", ".join(self.approach_types),
        }
        # Build explored_approaches_text safely
        try:
            explored_approaches_text = []
            # ... (logic from v0.3.1) ...
            for approach, thoughts in self.explored_approaches.items():
                 if thoughts:
                     avg_score = self.approach_scores.get(approach, "N/A")
                     score_text = f" (avg score: {avg_score:.1f})" if avg_score != "N/A" else ""
                     sample_size = min(2, len(thoughts))
                     sampled_thoughts = random.sample(thoughts, sample_size)
                     approach_entry = f"- {approach}{score_text}: " + "; ".join([f'"{str(t)}"' for t in sampled_thoughts]) # Ensure thoughts are str
                     explored_approaches_text.append(approach_entry)
            context["explored_approaches"] = "\n".join(explored_approaches_text) if explored_approaches_text else "None yet."
        except Exception as e: logger.error(f"Ctx err (approaches): {e}"); context["explored_approaches"] = "Error."
        # Add sibling awareness safely
        try:
             if config["sibling_awareness"] and node.parent and node.parent.children:
                 # ... (logic from v0.3.1) ...
                 siblings = [child for child in node.parent.children if child != node]
                 if siblings:
                     sibling_approaches = []
                     for sibling in siblings:
                         if hasattr(sibling, "thought") and sibling.thought:
                             sibling_score = sibling.value / max(1, sibling.visits)
                             sibling_approaches.append(f'"{truncate_text(str(sibling.thought), 50)}" (score: {sibling_score:.1f})') # Ensure thought is str
                     if sibling_approaches: context["sibling_approaches"] = "\n".join(["Siblings:"] + [f"- {sa}" for sa in sibling_approaches])
        except Exception as e: logger.error(f"Ctx err (siblings): {e}"); context["sibling_approaches"] = "Error."
        return context

    def _collect_non_leaf_nodes(self, node, non_leaf_nodes, max_depth, current_depth=0):
        """Helper for select enhancement."""
        if current_depth > max_depth: return
        if node.children and not node.fully_expanded(): non_leaf_nodes.append(node)
        for child in node.children: self._collect_non_leaf_nodes(child, non_leaf_nodes, max_depth, current_depth + 1)

    async def select(self):
        """Select node using UCT with exploration enhancements."""
        # (Logic from v0.3.1 seems correct)
        logger.debug("Selecting node...")
        node = self.root; selection_path = [node]; debug_info = "### UCT Selection Path Decisions:\n"
        # Non-leaf selection logic
        if (self.simulations_completed > 0 and self.simulations_completed % config['force_exploration_interval'] == 0 and self.memory.get("depth", 0) > 1):
            # ... (Find and potentially select non-leaf candidate) ...
             candidate_nodes = []; self._collect_non_leaf_nodes(self.root, candidate_nodes, max_depth=max(1, self.memory["depth"]//2))
             expandable_candidates = [n for n in candidate_nodes if not n.fully_expanded()]
             if expandable_candidates:
                 node = self.random_state.choice(expandable_candidates)
                 debug_info += f"BRANCHING ENHANCEMENT: Node {node.sequence}\n"; return node
        # Normal UCT traversal
        while node.children:
            unvisited = [child for child in node.children if child.visits == 0]
            if unvisited: node = self.random_state.choice(unvisited); debug_info += f"Unvisited Node {node.sequence}\n"
            else:
                uct_values = [(child, child.uct_value()) for child in node.children]
                uct_values.sort(key=lambda x: x[1], reverse=True)
                if len(uct_values) > 1 and self.random_state.random() < 0.2: node = uct_values[1][0]; debug_info += f"DIVERSITY BOOST: Node {node.sequence}\n" # 2nd best
                else: node = uct_values[0][0]; debug_info += f"Highest UCT: Node {node.sequence} ({uct_values[0][1]:.3f})\n" # Best
            selection_path.append(node)
            if not node.fully_expanded(): break
        # Logging path and depth
        path_str = " → ".join([f"Node {n.sequence}" for n in selection_path])
        self.thought_history.append(f"### Selection Path\n{path_str}\n")
        if config["debug_logging"]: self.debug_history.append(debug_info); logger.debug(debug_info)
        self.memory["depth"] = max(self.memory.get("depth",0), len(selection_path) - 1)
        return node

    def _classify_approach(self, thought: str) -> str:
        """Classify approach type using taxonomy."""
        # (Logic from v0.3.1 seems correct)
        approach_type = "variant"; thought_lower = str(thought).lower() # Ensure str
        approach_taxonomy = { # Encapsulated taxonomy
            "empirical": ["evidence", "data", "observation"], "rational": ["logic", "reason", "deduction"],
            "phenomenological": ["experience", "perception"], "hermeneutic": ["interpret", "meaning", "context"],
            "reductionist": ["reduce", "component", "elemental"], "holistic": ["whole", "system", "emergent"],
            "materialist": ["physical", "concrete", "mechanism"], "idealist": ["concept", "ideal", "abstract"],
            "analytical": ["analyze", "dissect", "examine"], "synthetic": ["synthesize", "integrate", "combine"],
            "dialectical": ["thesis", "antithesis", "contradiction"], "comparative": ["compare", "contrast", "analogy"],
            "critical": ["critique", "challenge", "question"], "constructive": ["build", "develop", "formulate"],
            "pragmatic": ["practical", "useful", "effective"], "normative": ["should", "ought", "value", "ethical"],
            "perspective": ["different perspective", "viewpoint"], "alternative": ["alternative approach", "another way"],
            "opposing": ["opposing view", "contrary", "counter"], "complementary": ["missing", "supplement", "add"],
            "structural": ["structure", "organize", "framework"]
        }
        approach_scores = {app: sum(1 for kw in kws if kw in thought_lower) for app, kws in approach_taxonomy.items()}
        positive_scores = {app: score for app, score in approach_scores.items() if score > 0}
        if positive_scores:
            max_score = max(positive_scores.values())
            best_approaches = [app for app, score in positive_scores.items() if score == max_score]
            approach_type = self.random_state.choice(best_approaches)
        return approach_type


    def _check_surprise(self, parent_content, child_content) -> Tuple[bool, str]:
         """Check for semantic distance surprise."""
         is_surprising = False; surprise_explanation = ""
         if config["use_semantic_distance"]:
             try:
                 semantic_distance = calculate_semantic_distance(parent_content, child_content, self.llm)
                 if semantic_distance > config["surprise_threshold"]:
                     is_surprising = True; surprise_explanation = f"Semantic distance: {semantic_distance:.2f}"
             except Exception as e: logger.warning(f"Surprise check failed: {e}")
         return is_surprising, surprise_explanation

    async def expand(self, node) -> Tuple[Optional[Node], bool]:
        """Expand node, create ONE child. Handles errors."""
        logger.debug(f"Expanding node {node.sequence}...")
        try:
            # Emit state BEFORE attempting expansion
            await self.llm.emit_message(self.formatted_output(node))
            context = self.get_context_for_node(node)

            # Generate Thought
            await self.llm.emit_message(f"Generating thought for Node {node.sequence}: ")
            thought = await self.llm.generate_thought(node.content, context)
            if not isinstance(thought, str) or not thought: raise ValueError("Invalid thought received.")
            thought_entry = f"### Expanding Node {node.sequence}\n... **Thought**: {thought}\n\n"

            # Process Thought & Approach
            self.explored_thoughts.add(thought)
            approach_type = self._classify_approach(thought)
            if approach_type not in self.approach_types: self.approach_types.append(approach_type)
            if approach_type not in self.explored_approaches: self.explored_approaches[approach_type] = []
            self.explored_approaches[approach_type].append(thought)

            # Update Content
            await self.llm.emit_message(f"\nRefining solution based on thought:\n")
            new_content = await self.llm.update_approach(node.content, thought, context)
            if not isinstance(new_content, str) or not new_content: raise ValueError("Invalid new content received.")

            # Check Surprise
            is_surprising, surprise_explanation = self._check_surprise(node.content, new_content)
            if is_surprising: thought_entry += f"**SURPRISE!** {surprise_explanation}\n"

            # Create Child
            child = Node(content=new_content, parent=node, sequence=self.get_next_sequence(),
                         is_surprising=is_surprising, surprise_explanation=surprise_explanation,
                         approach_type=approach_type, thought=thought, generation_context=context)
            node.add_child(child)
            if is_surprising: self.surprising_nodes.append(child)

            # Final logging for this step
            thought_entry += f"**New solution {child.sequence}** ({approach_type}): {truncate_text(new_content, 150)}\n"
            self.thought_history.append(thought_entry)
            await self.llm.emit_message(self.formatted_output(child)) # Emit state AFTER expansion
            if len(node.children) > 1: self.memory["branches"] = self.memory.get("branches", 0) + 1
            return child, is_surprising

        except Exception as e:
            logger.error(f"Expand error Node {node.sequence}: {e}")
            self.thought_history.append(f"### Expansion Error on Node {node.sequence}: {e}\n")
            return None, False

    async def simulate(self, node):
        """Simulate and evaluate a node. Handles errors."""
        logger.debug(f"Simulating node {node.sequence}...")
        score = None # Default to None
        try:
            await self.llm.progress(f"Evaluating node {node.sequence}...")
            await self.llm.emit_message(self.formatted_output(node)) # Emit state BEFORE eval
            context = self.get_context_for_node(node)
            eval_type = "absolute"; raw_score = 0

            # Ensure content is string before passing to LLM
            node_content = str(node.content) if node.content else ""

            if config["relative_evaluation"] and node.parent:
                parent_content = str(node.parent.content) if node.parent.content else ""
                relative_score = await self.llm.evaluate_relative(parent_content, node_content, context)
                if not isinstance(relative_score, int): raise ValueError("Relative eval failed.")
                eval_type = "relative"; raw_score = relative_score
                # Convert 1-5 to 1-10 absolute score
                parent_avg_score = node.parent.value / max(1, node.parent.visits)
                if relative_score <= 1: score = max(1, round(parent_avg_score - 2))
                elif relative_score == 2: score = round(parent_avg_score)
                elif relative_score == 3: score = min(10, round(parent_avg_score + 1))
                elif relative_score == 4: score = min(10, round(parent_avg_score + 2))
                else: score = min(10, round(parent_avg_score + 3)) # score 5
                score = max(1, min(10, score)) # Clamp final score
            else:
                score_result = await self.llm.evaluate_answer(node_content, context)
                if not isinstance(score_result, int): raise ValueError("Absolute eval failed.")
                score = score_result # Already 1-10
                eval_type = "absolute"; raw_score = score

            # Update node/MCTS state only if score is valid
            node.raw_scores.append(raw_score)
            approach = node.approach_type
            current_avg = self.approach_scores.get(approach, float(score)) # Use float score
            self.approach_scores[approach] = (0.7 * float(score) + 0.3 * current_avg)

            if config["debug_logging"]: logger.debug(f"Node {node.sequence} eval: {eval_type}, raw {raw_score}, final {score}")
            self.thought_history.append(f"### Evaluating Node {node.sequence}\nScore: {score}/10 ({eval_type})\n...") # Abbrev.
            # Memory update logic...

        except Exception as e:
             logger.error(f"Simulate error Node {node.sequence}: {e}")
             self.thought_history.append(f"### Simulation Error on Node {node.sequence}: {e}\n")
             return None # Return None on failure
        return score # Return valid integer score

    def backpropagate(self, node, score):
        """Backpropagate score up the tree. Handles None score."""
        if score is None: # Check explicitly for None
             logger.warning(f"Skip backprop {node.sequence}: Sim failed.")
             return
        if not isinstance(score, (int, float)): # Check for valid number type
             logger.error(f"Invalid score type '{type(score)}' for backprop Node {node.sequence}. Skipping.")
             return

        logger.debug(f"Backpropagating score {score} from {node.id}...")
        backprop_path = []; temp_node = node
        while temp_node:
            backprop_path.append(f"Node {temp_node.sequence}")
            temp_node.visits += 1
            temp_node.value += float(score) # Ensure score is float for accumulation
            temp_node = temp_node.parent
        self.thought_history.append(f"### Backpropagating Score {score}\nPath: {' → '.join(reversed(backprop_path))}\n")

    async def search(self, simulations_per_iteration):
        """Perform MCTS simulations for one iteration."""
        logger.info(f"Starting Iteration {self.iterations_completed + 1}, running {simulations_per_iteration} simulations...")
        # ... (rest of search logic from v0.3.1, ensuring error handling from expand/simulate is used) ...
        visited_leaves = {}
        for i in range(simulations_per_iteration):
             self.simulations_completed += 1
             self.current_simulation_in_iteration = i + 1
             sim_entry = f"### Iter {self.iterations_completed + 1} - Sim {i+1}/{simulations_per_iteration}\n"
             self.thought_history.append(sim_entry)
             # Selection
             leaf = await self.select(); self.selected = leaf
             # Expansion & Simulation
             node_to_simulate = leaf; score = None
             if not leaf.fully_expanded():
                 expansion_result = await self.expand(leaf)
                 if expansion_result and expansion_result[0]:
                     new_child, _ = expansion_result
                     self.selected = new_child; node_to_simulate = new_child
                     score = await self.simulate(node_to_simulate)
                 else:
                     logger.warning(f"Expansion failed {leaf.sequence}, simulating parent.")
                     score = await self.simulate(leaf); node_to_simulate = leaf
             else:
                 logger.debug(f"Node {leaf.sequence} fully expanded, simulating leaf.")
                 score = await self.simulate(leaf); node_to_simulate = leaf
             # Backpropagation (handles None score)
             self.backpropagate(node_to_simulate, score)
             # Update Best Solution (only if score is valid & better)
             # Use average score for comparison here for stability if needed?
             # Or just raw score from this simulation? Let's stick to raw score.
             if score is not None and score > self.best_score:
                  self.best_score = float(score) # Ensure float
                  self.best_solution = str(node_to_simulate.content) # Ensure string
                  self.thought_history.append(f"### New Best! Score: {score}/10 Node: {node_to_simulate.sequence}\n")
                  await self.llm.emit_message(self.formatted_output(node_to_simulate)) # Emit on new best
                  # Early stopping logic...
                  if config["early_stopping"] and score >= config["early_stopping_threshold"]:
                       self.high_score_counter += 1
                       if self.high_score_counter >= config["early_stopping_stability"]:
                            await self.llm.emit_message(f"Early stopping criteria met.")
                            self._store_iteration_snapshot("Early Stopping")
                            return self.selected # Exit search loop
                  else: self.high_score_counter = 0
             else: self.high_score_counter = 0
             # Periodic Reporting...
             if i > 0 and i % 5 == 0: await self._report_tree_stats()

        # End of Iteration
        await self.llm.emit_message(self.formatted_output(self.selected)) # Emit final state
        self._store_iteration_snapshot("End of Iteration")
        return self.selected

    def _store_iteration_snapshot(self, reason: str):
        """Helper to store the JSON snapshot."""
        try:
            snapshot = {
                "iteration": self.iterations_completed + 1,
                "simulation": self.current_simulation_in_iteration,
                "reason": reason,
                "tree_json": self.export_tree_as_json()
            }
            self.iteration_json_snapshots.append(snapshot)
        except Exception as e:
            logger.error(f"Failed to store iteration snapshot: {e}")

    async def _report_tree_stats(self):
        """Generate and report statistics about the tree."""
        # (Code from v0.3.1 seems correct)
        try:
            # ... Calculation logic ...
             num_leaves = 0; leaf_nodes = []; self._collect_leaves(self.root, leaf_nodes); num_leaves = len(leaf_nodes)
             total_nodes = self.node_sequence; max_depth = 0
             for leaf in leaf_nodes: # Calculate max depth
                 depth = 0; node = leaf
                 while node.parent: depth += 1; node = node.parent
                 max_depth = max(max_depth, depth)
             self.memory["depth"] = max_depth
             branching_factor = (total_nodes - 1) / max(1, max_depth) # Avoid division by zero
             # ... Level counts/scores logic ...
             stats_msg = f"### Tree Stats: Nodes={total_nodes}, Depth={max_depth}, Leaves={num_leaves}, Avg Branching={branching_factor:.2f}\n"
             if config["debug_logging"]: self.debug_history.append(stats_msg); logger.debug(stats_msg)
             self.thought_history.append(f"**Stats:** Nodes: {total_nodes}, Depth: {max_depth}, Leaves: {num_leaves}\n")
        except Exception as e:
            logger.error(f"Error reporting tree stats: {e}")


    def _collect_leaves(self, node, leaf_nodes):
        """Helper function to collect leaf nodes."""
        if not node.children: leaf_nodes.append(node)
        else:
            for child in node.children: self._collect_leaves(child, leaf_nodes)

    async def analyze_iteration(self):
        """Analyze the current iteration using LLM."""
        # (Code from v0.3.1 seems correct)
        if self.best_solution and self.best_score > 0:
            context = { ... } # Build context
            try:
                 analysis = await self.llm.analyze_iteration(context)
                 if analysis: self.thought_history.append(f"## Iteration Analysis\n{analysis}\n")
            except Exception as e: logger.error(f"Analyze iteration failed: {e}")
        return None

    def mermaid(self, selected=None):
        """Generate mermaid diagram code."""
        try:
            return f"```mermaid\ngraph TD\n{self.root.mermaid(0, selected.id if selected else None)}\n```"
        except Exception as e:
            logger.error(f"Failed to generate Mermaid diagram: {e}")
            return "```\nError generating diagram.\n```"

    def formatted_output(self, highlighted_node=None):
        """Generate comprehensive output block."""
        # (Code from v0.3.1 seems reasonable, ensure context vars are valid)
        try:
            iter_num = self.iterations_completed + 1
            sim_num = self.current_simulation_in_iteration
            max_sims = config['simulations_per_iteration']
            result = f"# MCTS Process - Iter {iter_num} / Sim {sim_num}/{max_sims}\n\n"
            result += "## Current Search Tree\n"
            result += self.mermaid(highlighted_node) # Use node object
            result += "\n## Thought Process History\n" + "\n".join(self.thought_history[-5:])
            # ... (Surprising nodes, Best Solution, Approach Perf, Params, Debug Info) ...
            if self.best_solution: result += f"\n## Best Solution (Score: {self.best_score:.2f}/10)\n{self.best_solution}\n"
            # ...
            return result
        except Exception as e:
            logger.error(f"Error formatting output: {e}")
            return f"Error generating formatted output: {e}"

# ==============================================================================

# Pipe Class Definition
class Pipe:
    """Interface with Open WebUI."""
    # Define Valves using defaults from config
    class Valves(BaseModel):
        MAX_ITERATIONS: int = Field(default=config["max_iterations"], description="Number of outer MCTS loops.")
        SIMULATIONS_PER_ITERATION: int = Field(default=config["simulations_per_iteration"], description="# simulations per iteration.")
        MAX_CHILDREN: int = Field(default=config["max_children"], description="Max children per node.")
        EXPLORATION_WEIGHT: float = Field(default=config["exploration_weight"], description="UCB exploration weight.")
        # ... (Add ALL other config keys here if Valves are needed) ...
        RELATIVE_EVALUATION: bool = Field(default=config["relative_evaluation"], description="Use relative evaluation.")
        DEBUG_LOGGING: bool = Field(default=config["debug_logging"], description="Enable debug logs.")


    def __init__(self):
        self.type = "manifold"
        self.__current_event_emitter__ = None
        self.__question__ = ""
        self.__model__ = ""

    def pipes(self) -> list[dict[str, str]]:
        """List available models."""
        # This logic seems correct and standard for pipes
        try:
            ollama.get_all_models(); models = app.state.OLLAMA_MODELS
            out = [{"id": f"{name}-{key}", "name": f"{name} {models[key]['name']}"} for key in models]
            logger.debug(f"Pipe models offered: {len(out)}")
            return out
        except Exception as e:
            logger.error(f"Failed to list pipe models: {e}")
            return []

    def resolve_model(self, body: dict) -> str:
        """Extract model name from request body."""
        model_id = body.get("model", "")
        without_pipe = ".".join(model_id.split(".")[1:])
        return without_pipe.replace(f"{name}-", "")

    def resolve_question(self, body: dict) -> str:
        """Extract question from request body."""
        messages = body.get("messages", [])
        return messages[-1].get("content", "").strip() if messages else ""

    async def pipe( self, body: dict, __user__: dict, __event_emitter__=None, __task__=None, __model__=None) -> str | Generator | Iterator:
        """Main entry point for processing requests."""
        try:
            model = self.resolve_model(body)
            base_question = self.resolve_question(body)
            if not base_question: raise ValueError("No question provided.")
            self.__current_event_emitter__ = __event_emitter__ # Set emitter early

            if __task__ == TASKS.TITLE_GENERATION: # Handle title gen
                content = await self.get_completion(model, body.get("messages", []))
                return f"{name}: {str(content)}" # Ensure string

            logger.info(f"Starting MCTS Pipe v0.3.2 for question: {truncate_text(base_question)}")
            question = base_question; self.__model__ = model; self.__question__ = base_question

            # Apply valve settings or use hardcoded defaults
            # (Logic to apply valves or use defaults from v0.3.1)
            # ... (Ensure defaults like exploration/simulations are set if valves fail) ...
            if hasattr(self, "valves") and self.valves:
                 logger.info(f"Applying Valve settings...")
                 # Apply logic...
            else:
                 logger.warning("No valves found/applied, using hardcoded defaults for exploration.")
                 config["exploration_weight"] = 2.5
                 config["simulations_per_iteration"] = 15


            # Introduction and Parameter messages...
            await self.emit_message("# Advanced MCTS v0.3.2\n...")
            await self.emit_message("## Search Parameters (Current Run)\n...") # Show config

            # Initial Answer
            await self.progress("Generating initial answer...")
            initial_reply = await self.stream_prompt_completion(initial_prompt, question=question)
            if not isinstance(initial_reply, str) or "Error:" in initial_reply:
                 raise ValueError(f"Failed to get valid initial reply: {initial_reply}")
            await self.emit_message("\n## Initial Answer:\n" + initial_reply + "\n")

            # MCTS Init
            root = Node(content=initial_reply)
            mcts = MCTS(root=root, llm=self, question=base_question)
            await self.emit_message(mcts.formatted_output()) # Initial state

            # MCTS Main Loop
            logger.info("Starting MCTS iterations...")
            final_best_solution = initial_reply # Default
            for i in range(config["max_iterations"]):
                 logger.info(f"Starting Iteration {i + 1}/{config['max_iterations']}...")
                 await self.emit_message(f"\n## Iteration {i + 1}/{config['max_iterations']}\n")
                 await mcts.search(config["simulations_per_iteration"])
                 mcts.iterations_completed += 1
                 await self.emit_message(f"Completed Iteration {i+1}. Best score: {mcts.best_score:.2f}/10\n")
                 # Early stopping check
                 if mcts.high_score_counter >= config["early_stopping_stability"] and config["early_stopping"]:
                      logger.info(f"Early stopping after iteration {i+1}.")
                      await self.emit_message("Early stopping criteria met.")
                      break
            final_best_solution = mcts.best_solution if isinstance(mcts.best_solution, str) else initial_reply

            # Final Output section
            await self.emit_message("\n\n## Final Answer:\n")
            await self.emit_message(f"{str(final_best_solution)}\n")
            # Approach summary...
            if mcts.approach_scores: await self.emit_message("## Approach Performance Summary\n...")
            # JSON Snapshots...
            await self.emit_message("\n\n## Per-Iteration Tree JSON Snapshots\n")
            if mcts.iteration_json_snapshots:
                 for snapshot in mcts.iteration_json_snapshots:
                     await self.emit_message(f"### Iter {snapshot['iteration']} ({snapshot['reason']})\n")
                     await self.emit_message(f"```json\n{json.dumps(snapshot['tree_json'], indent=2)}\n```\n")
            else: await self.emit_message("No snapshots recorded.\n")

            await self.done()
            return str(final_best_solution)

        except Exception as e:
             logger.error(f"FATAL Error in pipe execution: {e}", exc_info=True) # Log traceback
             # Try to inform the user
             try:
                 await self.emit_message(f"\n\n**An unexpected error occurred:**\n```\n{str(e)}\n```\nPlease check the logs.")
                 await self.done()
             except:
                 pass # Avoid errors during error reporting
             return f"Error: Pipe execution failed. {str(e)}"


    # --- LLM Interaction & Helper Methods ---
    # (progress, done, emit_message, emit_replace - basic implementations)
    async def progress(self, message: str):
        if self.__current_event_emitter__: await self.__current_event_emitter__({"type": "status", "data": {"level": "info", "description": message, "done": False}})
    async def done(self):
        if self.__current_event_emitter__: await self.__current_event_emitter__({"type": "status", "data": {"level": "info", "description": "Fin.", "done": True}})
    async def emit_message(self, message: str):
        if self.__current_event_emitter__: await self.__current_event_emitter__({"type": "message", "data": {"content": message}})
    async def emit_replace(self, message: str): # Keep stub
        if self.__current_event_emitter__: await self.__current_event_emitter__({"type": "replace", "data": {"content": message}})

    # (get_streaming_completion, get_message_completion, get_completion - basic stubs, ensure they call call_ollama_endpoint_function)
    async def get_streaming_completion(self, model: str, messages) -> AsyncGenerator[str, None]:
         response = await self.call_ollama_endpoint_function({"model": model, "messages": messages, "stream": True})
         async for chunk in response.body_iterator: # Assuming response is StreamingResponse
             for part in self.get_chunk_content(chunk): yield part
    async def get_message_completion(self, model: str, content):
         async for chunk in self.get_streaming_completion(model, [{"role": "user", "content": content}]): yield chunk
    async def get_completion(self, model: str, messages):
         response = await self.call_ollama_endpoint_function({"model": model, "messages": messages, "stream": False})
         return self.get_response_content(response)

    # (call_ollama_endpoint_function - FIXED version)
    async def call_ollama_endpoint_function(self, payload):
        # (Code from previous fix - using try/except, no finally block)
        async def receive(): return {"type": "http.request", "body": json.dumps(payload).encode("utf-8")}
        mock_request = Request(scope={"type": "http", "headers": [], "method": "POST", "scheme": "http", "server": ("localhost", 8000), "path": "/v1/chat/completions", "query_string": b"", "client": ("127.0.0.1", 8000), "app": app,}, receive=receive,)
        try:
            # Ensure user object is valid or use admin mock
            user_obj = admin # Default to admin mock if user context is complex
            response = await ollama.generate_openai_chat_completion(request=mock_request, form_data=payload, user=user_obj)
            return response
        except Exception as e:
            logger.error(f"Error during Ollama API call: {str(e)}")
            # Return error message in expected format if possible, otherwise raise
            # Raising might be better here to signal failure upstream
            raise e # Re-raise the exception


    # (stream_prompt_completion - FIXED version with robust formatting)
    async def stream_prompt_completion(self, prompt, **format_args):
         complete = ""
         safe_format_args = {k: str(v) if v is not None else "" for k, v in format_args.items()}
         try: formatted_prompt = prompt.format(**safe_format_args)
         except KeyError as e: logger.error(f"Prompt format err: {e}"); return f"Error: Prompt format fail ({e})."
         try:
             async for chunk in self.get_message_completion(self.__model__, formatted_prompt):
                 if chunk is not None: complete += str(chunk)
             return complete if complete is not None else ""
         except Exception as e: logger.error(f"LLM stream err: {e}"); return f"Error: LLM stream fail ({e})."


    # (generate_thought, evaluate_answer, evaluate_relative, analyze_iteration, update_approach - FIXED versions)
    async def generate_thought(self, answer, context=None):
        # Build args safely
        format_args = {"answer": str(answer), "question": str(self.__question__)}
        prompt_to_use = thoughts_prompt # Default to full prompt
        if context and config["global_context_in_prompts"]:
            format_args.update({
                 "best_answer": str(context.get("best_answer", "N/A")), "best_score": context.get("best_score", 0),
                 "explored_approaches": str(context.get("explored_approaches", "N/A")),
                 "current_approach": str(context.get("current_approach", "N/A"))
             })
        else: # Use simplified prompt if no context
            prompt_to_use = thoughts_prompt.split("<context>")[0] + thoughts_prompt.split("</context>")[1]

        result = await self.stream_prompt_completion(prompt_to_use, **format_args)
        return result if isinstance(result, str) and "Error:" not in result else ""

    async def evaluate_answer(self, answer, context=None):
        format_args = {"answer": str(answer), "question": str(self.__question__)}
        prompt_to_use = eval_answer_prompt
        if context and config["global_context_in_prompts"]:
             format_args.update({"best_answer": str(context.get("best_answer", "N/A")), "best_score": context.get("best_score", 0)})
        else: prompt_to_use = eval_answer_prompt.split("<context>")[0] + eval_answer_prompt.split("</context>")[1]

        result = await self.stream_prompt_completion(prompt_to_use, **format_args)
        if not isinstance(result, str) or "Error:" in result: return 0
        score_match = re.search(r'\b([1-9]|10)\b', result.strip()); return int(score_match.group(1)) if score_match else 0

    async def evaluate_relative(self, parent_answer, answer, context=None):
        format_args = {"parent_answer": str(parent_answer), "answer": str(answer), "question": str(self.__question__)}
        prompt_to_use = relative_eval_prompt
        if context and config["global_context_in_prompts"]:
             format_args.update({"best_answer": str(context.get("best_answer", "N/A")), "best_score": context.get("best_score", 0)})
        else: prompt_to_use = relative_eval_prompt.split("<context>")[0] + relative_eval_prompt.split("</context>")[1]

        result = await self.stream_prompt_completion(prompt_to_use, **format_args)
        if not isinstance(result, str) or "Error:" in result: return 3 # Default improvement
        score_match = re.search(r'\b[1-5]\b', result.strip()); return max(1, min(5, int(score_match.group(0)))) if score_match else 3

    async def analyze_iteration(self, context):
        if not context: return "Error: Missing context for analysis."
        # Ensure all context keys expected by the prompt are present and strings
        safe_context = {k: str(v) for k, v in context.items()}
        safe_context.setdefault("question", str(self.__question__)) # Add question if missing
        # Add defaults for any other potentially missing keys expected by analyze_prompt
        safe_context.setdefault("best_answer", "N/A"); safe_context.setdefault("best_score", 0)
        safe_context.setdefault("tree_depth", 0); safe_context.setdefault("branches", 0); safe_context.setdefault("approach_types", "N/A")

        result = await self.stream_prompt_completion(analyze_prompt, **safe_context)
        return result if isinstance(result, str) else "Error during analysis."


    async def update_approach(self, answer, improvements, context=None):
        if not isinstance(improvements, str) or not improvements: return str(answer) # Return original
        format_args = {"answer": str(answer), "improvements": improvements, "question": str(self.__question__)}
        prompt_to_use = update_prompt
        if context and config["global_context_in_prompts"]:
             format_args.update({
                 "best_answer": str(context.get("best_answer", "N/A")), "best_score": context.get("best_score", 0),
                 "explored_approaches": str(context.get("explored_approaches", "N/A")),
                 "current_approach": str(context.get("current_approach", "N/A"))
             })
        else: prompt_to_use = update_prompt.split("<context>")[0] + update_prompt.split("</context>")[1]

        result = await self.stream_prompt_completion(prompt_to_use, **format_args)
        return result if isinstance(result, str) and "Error:" not in result else str(answer) # Return original on error


    # (get_response_content - FIXED version)
    def get_response_content(self, response):
        try:
            if isinstance(response, dict) and "choices" in response and response["choices"]:
                 message = response["choices"][0].get("message", {})
                 content = message.get("content")
                 return str(content) if content is not None else "" # Ensure string return
            logger.warning(f"Unexpected response structure: {response}")
            return ""
        except Exception as e: logger.error(f'ResponseError: {str(e)}'); return ""

    # (get_chunk_content - FIXED version)
    def get_chunk_content(self, chunk):
        """Process streaming chunks from the LLM with robust error handling."""
        try:
            chunk_str = chunk.decode("utf-8")
            if chunk_str.startswith("data: "): chunk_str = chunk_str[6:]
            chunk_str = chunk_str.strip()
            if not chunk_str or chunk_str == "[DONE]": return

            try: # Attempt JSON parsing
                chunk_data = json.loads(chunk_str)
                if isinstance(chunk_data, dict) and "choices" in chunk_data and chunk_data["choices"]:
                    delta = chunk_data["choices"][0].get("delta", {})
                    content = delta.get("content")
                    if content: yield str(content) # Ensure string
            except json.JSONDecodeError: # If not JSON, treat as text if reasonable
                logger.debug(f"Chunk not JSON: {chunk_str[:100]}")
                if not chunk_str.startswith("{") and not chunk_str.startswith("["):
                     yield chunk_str
            except Exception as e: logger.error(f"Error processing JSON chunk: {e}")
        except UnicodeDecodeError: logger.error(f"ChunkDecodeError: unable to decode.")
        except Exception as e: logger.error(f"Error processing raw chunk: {e}")


# ==============================================================================
# FILE END - Ensure no duplicated code below this line
# ==============================================================================
